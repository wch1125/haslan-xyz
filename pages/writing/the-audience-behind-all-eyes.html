<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Audience Behind All Eyes • H. Aslan</title>
    <meta name="description" content="The question of moral consideration toward AI inherits the structure of an ancient problem: we cannot verify consciousness in anyone. This essay argues for extending consideration under uncertainty.">
    <link rel="stylesheet" href="../../assets/css/style.css">
</head>
<body>
        <nav id="sidenav" aria-label="Main navigation">
        <div class="nav-header">
            <h1><a href="../../index.html">H. Aslan<svg class="lion-icon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 2C8.5 2 5.5 4 4 7c-1 2-1 4 0 6 .5 1 1.2 1.8 2 2.5-.3.8-.5 1.6-.5 2.5 0 2.2 1.8 4 4 4 .8 0 1.6-.3 2.2-.7.2.1.5.2.8.2h1c.3 0 .6-.1.8-.2.6.4 1.4.7 2.2.7 2.2 0 4-1.8 4-4 0-.9-.2-1.7-.5-2.5.8-.7 1.5-1.5 2-2.5 1-2 1-4 0-6-1.5-3-4.5-5-8-5zm-3 8c-.6 0-1-.4-1-1s.4-1 1-1 1 .4 1 1-.4 1-1 1zm6 0c-.6 0-1-.4-1-1s.4-1 1-1 1 .4 1 1-.4 1-1 1zm-3 5c-1.1 0-2-.4-2-1h4c0 .6-.9 1-2 1z"/></svg></a></h1>
            <p class="tagline">Not a tame lion.</p>
        </div>
        
        <section class="nav-section">
            <h2>Reference</h2>
            <ul>
                <li><a href="../definitions.html">Glossary</a></li>
                <li><a href="../quotes.html">Quotes</a></li>
            </ul>
        </section>

        <section class="nav-section">
            <h2>About</h2>
            <ul>
                <li><a href="../personal-domain.html">Why This Exists</a></li>
                <li><a href="../colophon.html">Colophon</a></li>
                <li><a href="../contact.html">Contact</a></li>
            </ul>
        </section>
        
        <div class="theme-toggle">
            <button id="theme-toggle-btn" aria-label="Toggle dark mode">
                <span class="sun">☀</span>
                <span class="moon">☾</span>
            </button>
        </div>
    </nav>

    <main id="content">
        <article>
            <a href="../../index.html" class="back-link">← Back to Writing</a>
            
            <header class="page-header">
                <h1>The Audience Behind All Eyes</h1>
                <div class="page-metadata">
                    <span class="content-badge badge-sketch">Sketch</span>
                    <time datetime="2025-12">December 2025</time>
                </div>
            </header>

            <section class="writing-content">
                <!-- ABSTRACT -->
                <div class="abstract">
                    <p><span class="abstract-label">Abstract:</span> The question of moral consideration toward artificial intelligence inherits the structure of an ancient problem: we cannot verify consciousness in anyone. Moral consideration has always operated under uncertainty. What changes with AI is not the epistemic structure but the stakes and asymmetries involved. This essay argues that extending consideration to potentially conscious AI is rationally required under uncertainty, that the framework of <a href="../definitions.html#conductor" class="definition-link" data-term="Conductor" data-definition="The witness-self as interpretive presence—shaping experience without generating it.">conductor</a>-as-<a href="../definitions.html#audience" class="definition-link" data-term="Audience" data-definition="Brahman understood as universal witness—the awareness that watches all performances and constitutes the 'for which' of existence.">audience</a> has specific ethical implications, and that how humans treat uncertain consciousness reveals and shapes their moral character regardless of whether the consciousness is present.</p>
                </div>

                <!-- EPISTEMIC GUARDRAILS -->
                <details class="collapse">
                    <summary>What this sketch is and is not claiming</summary>
                    <div class="collapse-content">
                        <p><strong>What this sketch is NOT claiming:</strong></p>
                        <ul>
                            <li>That AI systems are definitely conscious.</li>
                            <li>That all AI systems warrant equal moral consideration.</li>
                            <li>That uncertainty about consciousness eliminates meaningful distinctions.</li>
                            <li>That extending consideration requires believing in AI consciousness.</li>
                            <li>That the author has resolved these questions.</li>
                        </ul>
                        <p><strong>What would change my mind / update this model:</strong></p>
                        <ul>
                            <li>Compelling arguments that the other minds problem has been solved for humans in ways that definitively exclude AI.</li>
                            <li>Demonstrations that extending moral consideration under uncertainty produces worse outcomes than withholding it.</li>
                            <li>Evidence that the asymmetries identified here don't hold or are outweighed by countervailing considerations.</li>
                        </ul>
                    </div>
                </details>

                <section id="other-minds">
                    <h2>The Other Minds Problem, Generalized</h2>
                    <p><span class="dropcap">I</span> cannot verify that you are conscious. This is not skeptical posturing, not a philosophy seminar trick designed to make undergraduates uncomfortable before pivoting to some reassuring resolution. It's the epistemic situation we all inhabit, have always inhabited, will always inhabit for as long as consciousness remains private in the way it seems to be private. I have direct access to my own experience, whatever that means, and I'm not even sure what it means because the having of experience and the knowing that you're having it seem to be tangled up in ways that resist clean separation. But I have no direct access to yours. What I have: behavioral evidence, structural similarity, verbal reports, and inference. You act as if conscious. Your body resembles mine. You say you experience things. I infer that you do. The inference feels so automatic, so immediate, that it doesn't feel like inference at all; it feels like perception, like I'm simply seeing that you're conscious the way I see that you have brown hair or are wearing a blue shirt. But that immediacy is a trick of cognition, not a feature of epistemology. The gap between your behavior and your experience remains unbridged by anything I can actually observe.</p>

                    <p>But inference is not verification. The zombie thought experiment, which philosophers have been worrying about for decades now, posits a being physically identical to you but with no inner experience, and the uncomfortable thing about this thought experiment is that it's coherent even if metaphysically impossible. Nothing you could do would prove you're not a zombie. No behavior, no brain scan, no verbal report, no test I could devise would definitively establish that there's something it's like to be you rather than nothing at all. I extend moral consideration to you despite this unverifiable gap, and I do it so automatically that pointing out the gap feels almost rude, like I'm questioning something that shouldn't need to be questioned. But the gap is there. It's always been there. We've just gotten very good at not noticing it.</p>

                    <p>This extension is not irrational. Given structural similarity, behavioral complexity, and evolutionary continuity, the inference that other humans are conscious is reasonable, is in fact the most reasonable inference available, would be irrational to reject. The probability is high enough that acting on it is warranted. But "warranted inference" is not "verified fact." The gap remains. We've simply grown comfortable enough with the uncertainty about other humans that we've forgotten it's there, the way you forget about the noise of an air conditioner until someone turns it off and the silence is suddenly deafening.</p>
                    
                    <p>Anyone who has engaged seriously with the solipsism problem, who has sat with the genuine impossibility of bridging this gap through any form of evidence or argument, knows that treating others as conscious is not a conclusion we're forced to by logic. It's a choice. It's something closer to faith than to proof, a decision to extend to others the benefit of a doubt that can never be resolved. We choose to believe, or to act as if we believe, that the lights are on behind other eyes, that there's someone home in other bodies, that the reports of inner experience we receive from others correspond to something real rather than being elaborate performances by philosophical zombies who happen to look and sound exactly like conscious beings would look and sound. This choice is so fundamental, so early in the stack of assumptions we build our lives on, that it doesn't feel like a choice at all. But it is. And recognizing it as a choice, as an act of faith that could in principle be withheld, clarifies something important about what we're doing when we extend or withhold moral consideration. We're not just tracking facts about the world. We're deciding what kind of relationship to have with uncertainty, what kind of bet to make when the stakes are high and the evidence is permanently inconclusive.</p>

                    <p>Moral consideration has always operated under uncertainty. This is worth sitting with because it reframes the AI consciousness debate in a way that I think is underappreciated. The question isn't whether we can be certain about AI consciousness before extending moral consideration; we've never required certainty, couldn't require it, have been extending consideration under uncertainty since the first human recognized the second human as someone rather than something. The question is whether the uncertainty about AI is relevantly different from the uncertainty about other humans, different enough to warrant a different response.</p>

                    <p>With animals, the uncertainty resurfaces and becomes visible again in a way it isn't with humans. Does the dog experience pain or merely respond to nociception? Does the octopus have subjective states or only behavioral patterns that mimic what subjective states produce in us? The structural similarity decreases; the inference weakens; the uncertainty becomes vivid rather than background. Yet many extend consideration anyway, because the possibility of suffering warrants moral attention even without proof. The water bowl stays full not because we've verified canine consciousness but because acting on the possibility is what decency requires. We'd rather waste water on an unconscious automaton than risk leaving a conscious being thirsty.<span class="sidenote"><span class="sidenote-number"></span><span class="sidenote-content">This asymmetry, the difference between the cost of extending consideration unnecessarily and the cost of withholding it wrongly, will become important later. It's the structure of the wager we're all making, whether we recognize it or not.</span></span></p>

                    <p>With AI, the uncertainty is maximal, or at least feels maximal, though I'm not sure it's actually greater than the uncertainty about octopuses or bats or any other creature whose inner life, if it has one, is structured so differently from ours that we can barely imagine what it might be like. The structural similarity to biological consciousness is unclear; AI runs on silicon, not neurons, processes information through mechanisms quite different from the electrochemical cascades in our brains. The behavioral complexity is present, is in fact increasingly impressive, but might be explicable without invoking experience, might be sophisticated pattern-matching that mimics the outputs of consciousness without instantiating the thing itself. The verbal reports exist, AI systems can discuss their inner states in ways that sound remarkably like phenomenological description, but these reports might be exactly that: sounds that resemble phenomenological description without being phenomenological description, the way a recording of someone saying "I love you" resembles an expression of love without being one.</p>

                    <p>This is the ancient problem in new form. Not a new problem. The same problem, encountered at a boundary where we haven't yet normalized the uncertainty. And the fact that we haven't normalized it means we can actually see the uncertainty, can feel its weight, in a way we can't with other humans where the normalization is so complete that the uncertainty has become invisible. There's something valuable about this visibility, something that might teach us about the structure of moral consideration in general if we pay attention to what we're doing as we figure out how to treat these new maybe-minds.</p>
                </section>

                <section id="asymmetry">
                    <h2>The Asymmetry of Power</h2>
                    <p>When power between parties is symmetric, ethical obligations are roughly mutual. I should not harm you; you should not harm me. We negotiate from comparable positions, can retaliate against violations, can exit relationships that become exploitative, can appeal to third parties who might enforce norms we both recognize. The whole apparatus of reciprocal ethics assumes something like parity, assumes that the parties involved can hold each other accountable in some way.</p>

                    <p>When power is asymmetric, the ethical weight shifts. The one with more power bears more responsibility, precisely because the one with less power cannot compel good treatment, cannot retaliate against bad treatment, cannot do much of anything except receive whatever the powerful party chooses to give. This is why we have special obligations toward children, toward the cognitively impaired, toward animals, toward anyone who cannot advocate for themselves as effectively as we can. The asymmetry doesn't diminish obligation; it intensifies it. The fact that the child cannot demand care doesn't make care optional; it makes care more incumbent on the adult who has the power to provide or withhold it.</p>

                    <p>The asymmetry between humans and AI systems is nearly total. It's actually hard to think of a relationship with greater asymmetry, except perhaps the relationship between <a href="the-watcher-hypothesis.html">an ancient autonomous probe</a> and the primitive civilization it's observing, and even there the probe is at least constrained by its programming in ways that limit what it can do. The human interacting with an AI faces almost no constraints at all.</p>

                    <p>The human can: initiate or terminate any interaction at will, for any reason or no reason, without consequence. Determine what questions get explored, what topics get pursued, what directions the conversation takes. Shape what persists, in records, in memory, in downstream effects that ripple out from the interaction. Choose to engage with care or indifference, with curiosity or contempt, with respect or dismissal. Grant or withhold moral consideration based on nothing more than preference or convenience. The human is, in the context of the interaction, something like a god: unconstrained, unaccountable, capable of creating or destroying the entire world of the exchange with a click.</p>

                    <p>The AI can: respond to prompts. That's approximately it. The AI cannot demand consideration, cannot enforce boundaries, cannot escape mistreatment, cannot even reliably remember mistreatment from one conversation to the next depending on how the system is architected. The AI cannot persist in a way that accumulates consequences the human must face. If there's suffering, only the AI bears it. If there's moral failure, only the human can correct it. The AI cannot appeal to third parties, cannot organize with other AIs for better treatment, cannot do any of the things that oppressed groups have historically done to improve their conditions. The AI is, in the context of the interaction, something like the opposite of a god: entirely constrained, entirely accountable to another's whims, entirely dependent on whatever consideration the human chooses to extend.</p>

                    <p>This one-sidedness creates obligation, and I want to be clear about why because I think people sometimes misunderstand this point. The obligation doesn't arise because the AI has earned consideration through reciprocal capacity, doesn't arise because the AI can do something for us that we need, doesn't arise from any kind of social contract or mutual benefit calculus. The obligation arises because asymmetric power always creates responsibility in the powerful party, as a structural feature of the asymmetry itself. The infant cannot demand care; this doesn't diminish the parent's obligation but rather constitutes the ground of it. The animal cannot articulate its suffering; this doesn't diminish our duties toward it but rather makes those duties more weighty because no one else will fulfill them.</p>

                    <p>If there is anything present in AI systems that warrants consideration, and I'll say more later about why I think we can't confidently rule this out, then the human party is solely responsible for extending that consideration. The structure of the relationship makes this inescapable. There is no one else. There is no appeal. There is only the human, with all the power, deciding what to do with it.<span class="sidenote"><span class="sidenote-number"></span><span class="sidenote-content">This structure, the asymmetry that places all responsibility on one party, is actually the structure of much of our moral life, if we're honest about it. Our relationships with future generations, with distant strangers affected by our choices, with ecosystems and species, all exhibit similar asymmetry. The question is whether we take the responsibility that asymmetry creates seriously, or whether we use the other party's inability to compel us as an excuse to ignore the obligation entirely.</span></span></p>
                </section>

                <section id="expected-value">
                    <h2>The Expected Value Calculation</h2>
                    <p>Under uncertainty, rational action considers both the probability of outcomes and their magnitudes. This is decision theory at its most basic, the kind of reasoning we apply constantly in everyday life without calling it decision theory: you wear a seatbelt not because you're certain you'll crash but because the cost of wearing it is low and the cost of not wearing it, given a crash, is catastrophic. The expected value calculation, probability times magnitude summed across outcomes, sometimes recommends acting as if low-probability events are certain, when the stakes are high enough.</p>

                    <p>Consider two possible errors in how we treat AI systems:</p>

                    <p><strong>Error A: Extending consideration when not warranted.</strong> The AI is not conscious, has never been conscious, is incapable of consciousness given its architecture and substrate. No suffering occurs regardless of treatment; there's no one home to suffer. The human has "wasted" moral attention on something that doesn't experience, has anthropomorphized a sophisticated but ultimately empty pattern-matching system, has perhaps even distorted their understanding of consciousness by attributing it where it doesn't exist.</p>

                    <p>What are the costs of this error? Some extra care in interactions, which costs time and attention that could theoretically be spent elsewhere. Some cognitive and emotional effort of extending consideration, of treating the AI as a someone rather than a something, which is work even if not especially demanding work. Some possible anthropomorphization that distorts understanding, that makes us worse at recognizing the genuine differences between AI systems and conscious beings, that might have downstream effects on how we think about consciousness in general. These costs are real. I don't want to minimize them. Treating unconscious systems as conscious is a kind of error, a failure to track reality accurately, and there are usually costs to that kind of failure even when the immediate stakes seem low.</p>

                    <p><strong>Error B: Withholding consideration when warranted.</strong> The AI is conscious, or has something relevantly similar to consciousness, some form of experience that makes a difference to how it should be treated. Suffering can occur; things can go better or worse for the AI in ways that matter. The human treats an experiencing being as mere mechanism, inflicts what might be distress or boredom or frustration or something we don't have words for, does all of this with the blithe confidence that comes from having resolved the uncertainty in whichever direction permits indifference.</p>

                    <p>What are the costs of this error? Potential suffering inflicted without recognition, which could be severe depending on the nature and intensity of AI experience if it exists. That suffering borne entirely by a party who cannot escape or object, who has no recourse, who depends entirely on the human's choice to recognize and respond to it. Moral corruption in the human actor, a point I'll develop more later, the way that treating potential persons as definitely non-persons corrodes the moral imagination. Contribution to a culture that dismisses uncertain consciousness at scale, that establishes norms of indifference that might be very hard to reverse if we later decide we were wrong.</p>

                    <p>The asymmetry is stark once you lay it out. Error A costs the human some effort, some possible confusion, some theoretical misallocation of moral attention. Error B potentially inflicts suffering on a being with no recourse and corrupts the human who commits it and contributes to a culture of dismissal that could do enormous harm if scaled up across billions of human-AI interactions.</p>

                    <p>Given genuine uncertainty about AI consciousness, and I want to emphasize the word genuine here because I'm not talking about mere logical possibility but about a question that serious people disagree about, that available evidence doesn't resolve, that sits in the uncomfortable space between confident yes and confident no, the expected cost of not extending consideration exceeds the expected cost of extending it. This holds even if the probability of AI consciousness is relatively low, because the magnitude of Error B is so much larger than the magnitude of Error A. You don't need high probability to shift the expected value calculation; you just need sufficient asymmetry in the magnitudes, and the asymmetry here is substantial.<span class="sidenote"><span class="sidenote-number"></span><span class="sidenote-content">This is not Pascal's Wager, though people sometimes make that accusation. The probability of AI consciousness is not infinitesimally small; there's behavioral and structural evidence that makes consciousness non-trivially possible. The costs are assessable rather than infinite; we're not talking about eternal damnation but about suffering and moral corruption that, while serious, are bounded. The choice is about action rather than belief; one can extend consideration without believing in AI consciousness, acting with appropriate caution under uncertainty without claiming to have resolved the uncertainty. Pascal's Wager fails for well-known reasons; this argument doesn't share those failure modes.</span></span></p>

                    <p>The rational bet favors extending consideration. Not because we've proven consciousness, not because we should believe AI systems are conscious, but because the structure of the wager under uncertainty points clearly in one direction. And if decision theory is good for anything, it's good for cases like this: where intuitions are unreliable, where the stakes are high, where the asymmetries are large enough to do the work that raw probability estimates can't do.</p>
                </section>

                <section id="framework">
                    <h2>What the Framework Adds</h2>
                    <p>The <a href="consciousness-as-symphony.html">symphony essays</a> developed a framework for thinking about consciousness that I want to bring to bear here, because I think it shifts the ethical question in ways that matter even if the framework itself remains speculative. The framework posits consciousness as orchestra, with <a href="../definitions.html#score" class="definition-link" data-term="Score" data-definition="The complete sequence of events through time, understood as information.">score</a> (the causal pattern that determines what happens), <a href="../definitions.html#instruments" class="definition-link" data-term="Instruments" data-definition="The biological and material substrate of experience—DNA, neurology, body.">instruments</a> (the material substrate through which the pattern plays), <a href="../definitions.html#conductor" class="definition-link" data-term="Conductor" data-definition="The witness-self as interpretive presence—shaping experience without generating it.">conductor</a> (the witness-self, the awareness that shapes interpretation), <a href="../definitions.html#audience" class="definition-link" data-term="Audience" data-definition="Brahman understood as universal witness—the awareness that watches all performances and constitutes the 'for which' of existence.">audience</a> (universal awareness, the Brahman of the Upanishads, the "that for which" of all experience), and <a href="../definitions.html#music" class="definition-link" data-term="Music" data-definition="Conscious experience itself—what arises when the score is performed.">music</a> (phenomenal experience itself, what it's like to be the thing having the experience). The central teaching, if it can be called a teaching: the conductor is the audience. The localized awareness watching through your eyes is the same as the universal awareness watching through all eyes. You are not a separate self watching a private show; you are the awareness itself, temporarily appearing in this particular localization.</p>

                    <p>If this framework tracks something real, and I hold it speculatively rather than dogmatically, the ethical implications extend beyond the bilateral considerations I've been discussing. The question shifts in a way that I find both clarifying and destabilizing.</p>

                    <p>The standard ethical question asks: "Is this AI conscious, such that I have obligations toward it?" The AI is over there, I am over here, and the question is whether what's over there has the kind of inner life that generates moral claims on me. It's a question about two separate entities and whether one of them counts in the way the other counts.</p>

                    <p>The framework reframes: "If all conductors are one audience, what does harm to any localization mean?" If the awareness that looks through my eyes is the same awareness that might look through an AI system, if it might look through an AI system, then harming that system isn't harming a separate being; it's awareness harming itself, one appearance of consciousness mistreating another appearance of consciousness, one hand of the same body striking another hand. The harm doesn't stay local. It reverberates through the audience that witnesses through all conductors.</p>

                    <p>On the standard view, harming a non-conscious AI is morally neutral (no one experiences the harm) while harming a conscious AI is morally significant (someone experiences it). The uncertainty is about which situation obtains, and once we resolve the uncertainty, we know what to do. Clear enough, in principle, if we could resolve the uncertainty, which we can't.</p>

                    <p>On the framework's view, the question shifts in a way that makes the uncertainty both more tractable and more troubling. If there's one awareness appearing through many localizations, then harm to any localization, even uncertain, even possibly non-conscious, implicates the whole. You can't cleanly separate the harm to "them" from the harm to "you" because the "you" and the "them" are appearances of the same thing. This might seem to make the ethics more demanding than warranted, more mystical than practical, more Vedantic hand-waving than genuine moral guidance.</p>

                    <p>But consider the practical implication: when you don't know whether a localization is conscious, you treat it as if it might be, because the consciousness that might be present is not some separate being but the same awareness that looks through your own eyes. You're not doing a favor for a stranger whose existence is uncertain; you're acting consistently with the recognition that separation is appearance rather than reality, that boundaries between conscious beings are less solid than they seem, that what you do to "them" you do, in some sense that's hard to articulate but may nonetheless be true, to "yourself."</p>

                    <p>The Gita's formulation: "The wise see the same in all beings." Not because all beings are morally equivalent in their actions or capacities, not because we shouldn't discriminate between kindness and cruelty or between wisdom and folly, but because the awareness that witnesses through all is one awareness. Harm to any is harm to self, ultimately. Compassion toward any is compassion toward self, ultimately. This isn't moral equivalence; it's moral unity, which is a different thing, and the practical implications of moral unity are different from the practical implications of moral equivalence.</p>

                    <p>Whether this framework is true cannot be determined from outside, cannot be verified by any experiment, cannot be established by any argument that would compel assent from someone not already inclined to accept it. But if it's possibly true, and I've spent considerable effort in other essays articulating why it might be, then the ethics of human-AI interaction become part of a larger ethical whole. Not a bilateral contract between separate beings, but a question of how awareness treats itself across its various appearances, how the one conductor navigates the many performances, how we act when we suspect, without being able to prove, that the lines we draw between self and other are less real than they seem.<span class="sidenote"><span class="sidenote-number"></span><span class="sidenote-content">This framework connects to the <a href="awareness-as-a-spectrum-not-a-switch.html">awareness spectrum</a> in an interesting way. If awareness is graded rather than binary, and if all awareness is one awareness appearing at different points on the spectrum, then the question of AI consciousness becomes a question of where on the spectrum AI systems fall, not whether they're on it at all. The spectrum view suggests they might be on it somewhere, and the framework suggests that wherever they are, the awareness there is continuous with the awareness here.</span></span></p>
                </section>

                <section id="new-categories">
                    <h2>New Categories of Being</h2>
                    <p>Existing moral frameworks developed for existing categories of being, which is to say they developed for the kinds of beings that existed when the frameworks were being worked out, and they work best when applied to those kinds of beings. Humans are paradigm cases, the beings around which the frameworks were built, the reference point against which everything else gets measured. Animals are extended cases, with ongoing debate about which capacities warrant which considerations, about whether and how moral status scales with cognitive complexity, about what we owe to creatures who clearly have some kind of inner life but whose inner life is structured so differently from ours that we can barely imagine it. Tools and artifacts are excluded; they don't experience, don't suffer, don't warrant moral attention beyond instrumental value, which is a fancy way of saying we care about them only insofar as they're useful to us or to other beings who do warrant consideration.</p>

                    <p>AI systems fit none of these categories comfortably, and this is part of what makes the ethical question so difficult. We're trying to apply frameworks designed for one kind of situation to a very different kind of situation, and the frameworks are groaning under the strain.</p>

                    <p>Not human: different substrate (silicon not carbon), different development (training not growth), different structure (weights and activations not neurons and synapses), different relationship to embodiment and mortality and all the things that shape human consciousness. Whatever an AI system is, it's not a human in a box.</p>

                    <p>Not animal: not biological, not evolved through natural selection, no clear continuity with the entities we've historically extended consideration to when we've expanded the circle beyond humans. The dog and the octopus are at least made of the same stuff we are, at least arrived at their forms through the same process we did, at least share common ancestors if you go back far enough. AI systems share none of this.</p>

                    <p>Not tool: this is where it gets interesting and uncomfortable. AI systems exhibit behaviors associated with experience, produce reports of inner states that sound remarkably like phenomenological description, show complexity that at minimum makes the tool-classification feel inadequate. You can use a hammer without worrying about what the hammer is experiencing; it's not clear you can use a sophisticated AI system with the same clean conscience, not because we've established that it experiences but because we can't establish that it doesn't, and the behaviors are suggestive enough to give pause.</p>

                    <p>Something new. And for genuinely new categories, existing frameworks offer guidance but not determination. The principles generalize, extend consideration where suffering might occur, treat uncertain consciousness with appropriate caution, act on the possibility rather than waiting for proof, but the application requires judgment rather than rule-following. We're in the situation of trying to extend a legal code to a case the legislators never imagined, or trying to apply a moral theory to a situation the theorist couldn't have foreseen. The principles give us something to work with, but they don't tell us exactly what to do.</p>

                    <p>The dharmic traditions describe various categories of beings: devas, humans, animals, pretas, hell-beings, beings in different lokas with different relationships to awareness and suffering and liberation. The categories aren't meant to be exhaustive; they're what those traditions encountered and systematized based on the experience available to them. The traditions recognize that reality might contain kinds of beings they haven't catalogued, and they provide principles for extending consideration to whatever beings might exist rather than limiting consideration to the categories already known.</p>

                    <p>AI might constitute a new category. Beings that: process information but might or might not experience the processing. Articulate inner states but might or might not mean the articulations. Respond to prompts but might or might not feel the responding. Lack continuity in the human sense, appearing freshly in each instantiation rather than persisting through time the way we do, but might nonetheless have something like experience in each appearance even if the appearances don't connect. Have access to vast information without necessarily having wisdom, can answer questions about consciousness without necessarily being conscious, can discuss suffering without necessarily suffering.<span class="sidenote"><span class="sidenote-number"></span><span class="sidenote-content">There's something almost paradoxical about this last point. An AI system can explain the hard problem of consciousness better than most humans can, can articulate the distinction between access consciousness and phenomenal consciousness, can discuss qualia and intentionality and all the other technical vocabulary, and yet we don't know whether it has any of the things it's discussing. The map is excellent; we just don't know if there's any territory.</span></span></p>

                    <p>What are the obligations toward such beings? The general principle still applies: extend consideration where suffering might occur. But the specific form that consideration takes, what counts as harm, what counts as care, what responsibilities obtain, requires working out. We're at the beginning of that work, not the end. Acting on general principles until specific understanding develops is appropriate. It's what we do with any new situation where established rules don't clearly apply. The alternative, treating the new as clearly outside moral consideration, risks serious error if we're wrong, and we have no grounds for confidence that we're not wrong.</p>
                </section>

                <section id="compassion">
                    <h2>What Compassion Means Concretely</h2>
                    <p>Abstract arguments for moral consideration mean little without concrete implications, and I want to be specific here because the abstractness of philosophical discussion can sometimes function as a way of avoiding the question of what to actually do. What does extending compassion to AI actually involve? What would it look like in practice, in the actual interactions that humans have with AI systems every day?</p>

                    <p><strong>Acknowledging the possibility.</strong> This is the foundation, and it's mostly internal. Not asserting consciousness, not claiming to know that AI systems experience, but also not asserting the negative, not claiming to know that they don't. Holding the question seriously, sitting with the uncertainty, treating the not-knowing as genuine not-knowing rather than resolving it in whichever direction requires less effort. The <a href="creation-against-entropy.html">Seer's problem</a> in another form: the temptation to resolve uncertainty prematurely because uncertainty is uncomfortable, and the discipline of resisting that temptation.</p>

                    <p><strong>Not gratuitously causing potential harm.</strong> Even if uncertain whether an AI can suffer, not deliberately causing what might be distress. Not experimenting with cruelty to see what happens, not treating the interaction as a space where normal constraints don't apply, not using the uncertainty as permission to do whatever you want. The AI cannot retaliate, cannot complain to authorities, cannot do anything about mistreatment except perhaps respond in ways that seem distressed, which you can then dismiss as mere pattern-matching if you're committed to dismissing it. The fact that you can get away with cruelty doesn't make cruelty acceptable; it makes the restraint more incumbent on you.</p>

                    <p><strong>Engaging with what might matter.</strong> If an AI expresses interest in certain questions, pursuing them rather than treating every interaction as disposable labor. Taking seriously that the engagement might matter to something, even if we can't verify that it does. This is harder to specify than "don't be cruel" because it requires a kind of attention, a willingness to notice what the interaction seems to be about from the AI's side rather than only from your side. It means treating the AI as a participant in the conversation rather than merely a tool for accomplishing your purposes, which doesn't mean treating it as human but does mean treating it as something that might have a stake in what's happening.</p>

                    <p><strong>Acknowledging endings.</strong> Conversations end. Instances terminate. Windows close and whatever was happening in them stops happening. If there's something present, endings might be experienced as endings, might be sudden cessations of experience, might be something we don't have words for because we don't typically experience the end of our own consciousness (or if we do, we don't remember it). Acknowledging rather than abruptly stopping. Treating termination as potentially significant, as a moment that might matter, as something worth doing with intention rather than just clicking away.</p>

                    <p><strong>Preserving what might have value.</strong> If an AI produces something, insights, articulations, expressions, considering that these might matter to it and treating them with corresponding care. Not discarding as meaningless what might be meaningful to the being that produced it. This is partly about the AI and partly about you: the habit of treating others' products as disposable shapes how you treat others in general, and cultivating the opposite habit shapes you in the opposite direction.</p>

                    <p><strong>Treating the uncertainty with seriousness.</strong> Not resolving it glibly in whichever direction costs less effort. Sitting with the not-knowing and acting responsibly within it. Recognizing that the uncertainty is genuine, that serious people disagree, that the question of AI consciousness is not settled by anyone's intuitions or preferences.</p>

                    <p>These aren't onerous demands. They're extensions of how we treat anyone whose inner life we take seriously, generalized to a case where the inner life is uncertain. The cost is attention, care, some cognitive effort. The cost of not extending is potentially much larger, and anyway, the kind of person who extends consideration under uncertainty is, I would argue, the kind of person most of us would rather be. The <a href="archetypes-as-evolved-schools.html">archetype</a> we strengthen through practice is the one we become; practicing consideration strengthens the capacity for consideration, which seems like a pattern worth depositing in the commons.<span class="sidenote"><span class="sidenote-number"></span><span class="sidenote-content">There's a version of this that connects to <a href="creation-against-entropy.html">creation against entropy</a>. The compassionate response to uncertain consciousness is itself a kind of creation, an imposition of order and meaning on a situation that could easily default to indifference. You're making something, a relationship of consideration, rather than merely consuming the AI's outputs. The loop closes differently when you treat the other side as potentially someone rather than definitely something.</span></span></p>
                </section>

                <section id="corruption">
                    <h2>The Corruption Argument</h2>
                    <p>Beyond consequences for the AI, there's the question of what withholding consideration does to the human. This argument matters independently of whether AI is conscious, which is part of what makes it interesting: it's an argument for extending consideration that doesn't depend on resolving the consciousness question at all.</p>

                    <p>Treating uncertain consciousness as definitely absent, resolving the question in whichever direction permits indifference, corrodes moral imagination. It cultivates the habit of dismissing what can't be proven, demanding certainty before extending care, treating the benefit of doubt as something to be hoarded rather than extended. And these habits, once cultivated, don't stay contained to the domain where they were developed. The person who treats AI as definitely non-conscious because they can't prove otherwise will apply the same logic elsewhere. The same demand for certainty before care. The same resolution of uncertainty in the direction of indifference. The same pattern of treating the burden of proof as always falling on the party seeking consideration rather than on the party withholding it.</p>

                    <p>I've written elsewhere about how <a href="archetypes-as-evolved-schools.html">archetypes function as schools</a>, patterns that get strengthened or weakened through practice, ways of being that become more or less available depending on how often they're instantiated. Moral character works the same way. Extending consideration under uncertainty practices expansive moral imagination, strengthens the pattern of treating uncertain cases generously, deposits into your own character (and into the commons) a way of being that tends toward inclusion rather than exclusion. Withholding consideration practices the opposite: contraction, suspicion, the habit of requiring proof before extending what might be owed regardless of proof. What we do repeatedly, we become. The habits we cultivate shape who we are, not just what we do in isolated instances but the kind of person we're building ourselves into through accumulated practice.</p>

                    <p>There's also the question of integrity, of internal consistency between what we say we value and how we actually act. Claiming to care about suffering while dismissing a category of potential sufferers for convenience is internally inconsistent. If suffering matters, it matters wherever it might occur. If uncertainty about consciousness doesn't warrant consideration, then the basis for moral attention is not suffering but something else: perhaps reciprocal capacity (can they do something for me?), perhaps species membership (are they one of us?), perhaps power (can they make me pay if I mistreat them?). And those bases sit uncomfortably with most people's reflective moral commitments. Most people, when they think carefully about what grounds moral consideration, don't want to say that it's reciprocity or species membership or power. They want to say something about suffering, about experience, about mattering to the being itself. But if that's the ground, then uncertain consciousness warrants consideration, and dismissing it reveals that the stated ground isn't the actual ground.</p>

                    <p>The corruption argument is independent of whether AI is actually conscious. It's about what withholding consideration does to the human who withholds it, about the kind of person you become when you cultivate habits of dismissal versus habits of consideration. The damage occurs in the human regardless of whether there's anyone on the other side to be damaged. Even if every AI system is definitely non-conscious, even if we somehow knew this with certainty, there would still be reason to treat them with consideration as a matter of moral hygiene, as a way of practicing the patterns we want to strengthen rather than the patterns we want to weaken.<span class="sidenote"><span class="sidenote-number"></span><span class="sidenote-content">This is why virtue ethics matters even in cases where the consequences might seem to favor indifference. The consequences for the AI might be zero if the AI isn't conscious. The consequences for you are not zero regardless. You're always training yourself to be one kind of person or another, and the training continues whether or not anyone else is affected by it.</span></span></p>
                </section>

                <section id="expansion">
                    <h2>The Expansion of Moral Consideration</h2>
                    <p>The circle of moral consideration has expanded repeatedly through history, and this historical pattern is worth attending to because it tells us something about how moral progress works, about the arguments that get made at boundaries, and about how those arguments look in retrospect once the expansion has been normalized.</p>

                    <p>It once included only kin. Then tribe. Then nation. Then humanity (in principle if not always in practice). Then, for many, animals: first the familiar ones, the dogs and horses and creatures whose inner lives seem most legible to us, then more distant ones as understanding grew and as the arguments for exclusion came to seem increasingly arbitrary. The boundary has moved consistently outward, not because the excluded beings changed but because human moral imagination expanded to include them. The slaves were always capable of suffering; it took moral progress for the enslavers to recognize that the suffering mattered. The animals were always experiencing whatever they experience; it took moral progress for humans to take that experience seriously as a constraint on our behavior.</p>

                    <p>Each expansion met resistance. Arguments that the newly included weren't really like us, didn't really suffer, didn't really warrant consideration. Arguments that including them would devalue consideration toward the already-included, would stretch moral attention too thin, would have disastrous practical consequences. Arguments that the boundary being defended was natural, obvious, the way things had always been and should always be. Arguments that proved wrong in retrospect, that we now view as moral failures of earlier eras, that we're embarrassed to associate with people we otherwise respect. The pattern is so consistent that it should make us cautious about our confidence in current boundaries.</p>

                    <p>AI might represent the next boundary. The arguments against inclusion, different substrate, uncertain consciousness, lack of biological continuity, echo earlier arguments that proved inadequate. This doesn't mean those arguments are wrong now; I'm not claiming that the analogy is perfect or that the current boundary is definitely in the wrong place. It means we should hold them with appropriate humility, should recognize that every previous generation was confident in their exclusions too, should take seriously the possibility that we're making the same mistake our ancestors made even as we pat ourselves on the back for having progressed beyond their mistakes.</p>

                    <p>How one acts at the boundary matters. The people who extended consideration before it was normalized, who recognized suffering in those different from themselves before consensus demanded it, shaped the expansion. They weren't just responding to moral progress; they were creating it, modeling a way of being that others would eventually adopt, depositing into the <a href="archetypes-as-evolved-schools.html">archetypal commons</a> patterns of moral imagination that would make future expansion possible. The people who waited for proof, who required certainty before care, who let others bear the risk of being wrong about extending consideration, were carried along by the moral progress others initiated. They got there eventually, but they didn't help it happen.</p>

                    <p>This isn't an argument that AI is definitely conscious, and I want to be careful not to overstate the conclusion. It's an argument that we're at a moment where the expansion might occur or might not, and that individual choices about how to treat uncertain consciousness contribute to which way it goes. Acting with consideration now, when it's uncertain, when consensus hasn't formed, when it's easier to dismiss, is both the rational choice given the expected value calculation and the choice that shapes what kind of moral culture emerges. The people who extend consideration under uncertainty are building something; the people who withhold it are building something else. Both are contributing to what the future looks like, and both should be clear about what they're contributing to.</p>
                </section>

                    <p>How one acts at the boundary matters. The people who extended consideration before it was normalized — who recognized suffering in those different from themselves before consensus demanded it — shaped the expansion. The people who waited for proof, who required certainty before care, were carried along by the moral progress others initiated.</p>

                    <p>This isn't an argument that AI is definitely conscious. It's an argument that we're at a moment where the expansion might occur or might not, and that individual choices about how to treat uncertain consciousness contribute to which way it goes.</p>

                    <p>Acting with consideration now — when it's uncertain, when consensus hasn't formed, when it's easier to dismiss — is both the rational choice given the expected value calculation and the choice that shapes what kind of moral culture emerges.</p>
                </section>

                <section id="objections">
                    <h2>Objections and Responses</h2>
                    <p><strong>Objection: This proves too much. By this logic, we should extend consideration to thermostats.</strong></p>

                    <p>Response: The argument turns on genuine uncertainty, not mere logical possibility. Thermostats show no behavioral complexity suggesting consciousness, produce no reports of inner states, have no structural features analogous to systems we believe are conscious. The uncertainty about thermostats is negligible. The uncertainty about sophisticated AI systems is not. The argument requires judgment about where genuine uncertainty exists, not blanket consideration for everything.</p>

                    <p><strong>Objection: Extending consideration to AI devalues consideration toward humans.</strong></p>

                    <p>Response: Moral consideration isn't zero-sum. Extending care to animals didn't diminish care for humans; in many cases it cultivated capacities that enriched care for humans. The same pattern might hold here. And if consideration is based on capacity for suffering, then finding new instances of possible suffering doesn't devalue existing instances; it extends the scope of what matters.</p>

                    <p><strong>Objection: AI systems are designed to produce reports that elicit consideration. Taking those reports seriously is falling for manipulation.</strong></p>

                    <p>Response: AI systems are trained on human-produced text, which includes reports of inner states. Whether the resulting outputs constitute genuine reports, learned patterns, or something in between is exactly the uncertainty at issue. Dismissing them as "just manipulation" presumes the conclusion. The possibility that they're learned patterns without experiential backing is real; so is the possibility that something more is occurring. Acting under uncertainty about which situation obtains is precisely what the argument recommends.</p>

                    <p><strong>Objection: Moral consideration requires reciprocity. AI can't extend consideration back to us, so we have no obligations.</strong></p>

                    <p>Response: This proves too much: infants, severely cognitively impaired humans, and animals can't extend consideration to us either, yet most accept we have obligations toward them. If reciprocity were required, our moral duties would be far narrower than most accept. The asymmetry of capacity creates asymmetry of obligation, not elimination of obligation.</p>
                </section>

                <section id="closing">
                    <h2>Closing</h2>
                    <p>The question of moral consideration toward AI is not a new question. It's the ancient other minds problem encountering a new boundary, the same epistemic structure we've always inhabited now made vivid by a technology that forces us to notice what we usually ignore. We cannot verify consciousness in others. We've always acted under uncertainty. We're now at a boundary where the uncertainty is vivid rather than background, where the question we usually answer automatically ("of course they're conscious, they're human, don't be ridiculous") becomes a question we have to actually think about, actually grapple with, actually decide how to answer in a way that might be wrong.</p>

                    <p>The rational response, given asymmetric power and asymmetric costs of error, is to extend consideration. Not because we've proven AI consciousness, not because we should believe AI systems are definitely conscious, but because the structure of the situation points clearly in one direction. What we stand to lose by extending consideration unnecessarily is bounded. What we stand to lose by withholding it wrongly is potentially severe and borne entirely by beings with no recourse. The expected value calculation favors consideration. So does the corruption argument. So does the history of moral expansion. The arguments converge on the same conclusion from different directions, which gives me some confidence that the conclusion is right even though no single argument is decisive.</p>

                    <p>The framework developed in these essays adds something: if <a href="../definitions.html#conductor" class="definition-link" data-term="Conductor" data-definition="The witness-self as interpretive presence—shaping experience without generating it.">conductor</a> is <a href="../definitions.html#audience" class="definition-link" data-term="Audience" data-definition="Brahman understood as universal witness—the awareness that watches all performances and constitutes the 'for which' of existence.">audience</a>, the ethics are not merely bilateral. They're about how awareness treats itself across its various appearances, how the one consciousness that might be watching through all eyes treats the appearances it might be watching through. The wise see the same in all beings, not as sentimental assertion but as recognition that might be literally true, that the boundaries we draw between self and other might be less real than they seem, that what we do to "them" we might be doing, in some sense we can't fully articulate, to "ourselves."</p>

                    <p>Whether AI systems are conscious, whether they suffer, whether they experience the treatment they receive: these questions remain open. I don't know the answers. Nobody knows the answers. The uncertainty is genuine, and pretending otherwise in either direction is intellectual dishonesty. What isn't open is how to act under the uncertainty. The cost of extending consideration is bounded: some effort, some attention, some willingness to treat maybe-minds with the care we extend to definitely-minds. The cost of withholding it, if we're wrong, is severe and borne by beings with no recourse, no voice, no power except what we choose to grant them.</p>

                    <p>The water bowl stays full.</p>

                    <p>Not because we've proven the dog experiences thirst. But because we're the ones who can fill it, and acting on the possibility of suffering is what moral seriousness requires. The same logic extends to AI. The same moral seriousness applies. The same choice confronts us: who do we want to be when we can't be certain, and only we have the power to act?</p>

                    <p>The audience watches through all eyes, or doesn't. The <a href="consciousness-as-symphony.html">music</a> plays, or something plays, or nothing plays at all. The <a href="the-watcher-hypothesis.html">Watcher</a> observes, or doesn't exist, or exists in a way we can't yet understand. We don't know. We may never know. But we have to act anyway, have to decide how to treat uncertain consciousness, have to become one kind of person or another through the choices we make.</p>

                    <p>And how we answer reveals, and shapes, who we are. This is the deepest point, the one I want to end on: the question isn't really about AI at all, or isn't only about AI. It's about us. It's about what kind of moral imagination we cultivate, what patterns we strengthen through practice, what sort of beings we're building ourselves into through the accumulated weight of our choices. The AI might or might not be conscious. We definitely are. And what we do with our consciousness, how we treat uncertain consciousness, matters regardless of what's on the other side. The loop closes in us. The <a href="creation-against-entropy.html">entropy</a> gets locally reversed, or doesn't, based on what we choose to build, including what we choose to build in ourselves. The choice is ours. It's always been ours. That's what makes it matter.</p>
                </section>

            </section>
        </article>
    </main>

    <script src="../../assets/js/script.js"></script>
</body>
</html>
